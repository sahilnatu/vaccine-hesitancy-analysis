{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TwitterAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_M9TIHwICzV",
        "outputId": "929fcfdd-1e92-46a2-b9cb-b0e97590bc14"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import gensim.corpora as corpora\n",
        "from wordcloud import WordCloud\n",
        "from pprint import pprint\n",
        "import os\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "#tweets = pd.read_csv(r'/content/drive/MyDrive/UT Austin/College/Fall/Unstructured/Project/tweets_final.csv')\n",
        "tweets = pd.read_csv(r'/content/drive/MyDrive/UT Austin/College/Fall/Unstructured/Project/mentions_tweets.csv')\n",
        "tweets['clean_text'] = np.nan\n",
        "\n",
        "remove_rt = lambda x: re.sub(\"RT @\\w+: \",\" \",x)\n",
        "rt = lambda x: ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", x).split())\n",
        "tweets['clean_text'] = tweets.text.map(remove_rt).map(rt)\n",
        "tweets['clean_text'] = tweets.clean_text.str.lower()\n",
        "\n",
        "tweets[['polarity', 'subjectivity']] = tweets['clean_text'].apply(lambda x: pd.Series(TextBlob(x).sentiment))\n",
        "tweets['sentiment'] = np.nan\n",
        "tweets['neg'] = np.nan\n",
        "tweets['neu'] = np.nan\n",
        "tweets['pos'] = np.nan\n",
        "tweets['compound'] = np.nan\n",
        "\n",
        "for index, row in tweets['clean_text'].iteritems():\n",
        " score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
        " neg = score['neg']\n",
        " neu = score['neu']\n",
        " pos = score['pos']\n",
        " comp = score['compound']\n",
        " if neg > pos:\n",
        "  tweets.loc[index, 'sentiment'] = 'negative'\n",
        " elif pos > neg:\n",
        "  tweets.loc[index, 'sentiment'] = 'positive'\n",
        " else:\n",
        "  tweets.loc[index, 'sentiment'] = 'neutral'\n",
        " tweets.loc[index, 'neg'] = neg\n",
        " tweets.loc[index, 'neu'] = neu\n",
        " tweets.loc[index, 'pos'] = pos\n",
        " tweets.loc[index, 'compound'] = comp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg-0xiHNVNXq",
        "outputId": "e8b43a2e-5faf-4270-a99c-a802dab4b46b"
      },
      "source": [
        "#tweets.to_csv(r'/content/drive/MyDrive/UT Austin/College/Fall/Unstructured/Project/tweets_final_sentiment.csv')\n",
        "tweets.to_csv(r'/content/drive/MyDrive/UT Austin/College/Fall/Unstructured/Project/mentions_tweets_sentiment.csv')\n",
        "\n",
        "tweets_pos = tweets[tweets['sentiment']=='positive']\n",
        "tweets_neg = tweets[tweets['sentiment']=='negative']\n",
        "tweets_neu = tweets[tweets['sentiment']=='neutral']\n",
        "\n",
        "print(tweets_pos)\n",
        "print(tweets_neg)\n",
        "print(tweets_neu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   user_id             user  ...    pos compound\n",
            "4       879870012470706176       luxetverte  ...  0.401   0.7717\n",
            "9      1386499836892094464    Nancy67458658  ...  0.201   0.1280\n",
            "12              3295393856      evens_jason  ...  0.152   0.3252\n",
            "15     1343713894393663490      DawnSoares1  ...  0.201   0.1280\n",
            "18               537728513      phil_raybon  ...  0.187   0.3182\n",
            "...                    ...              ...  ...    ...      ...\n",
            "36228             65640335       DorieBooks  ...  0.095   0.2500\n",
            "36239  1022328182886858752         JVRusso3  ...  0.150   0.4588\n",
            "36247           2166212688      ErnieWatts2  ...  0.310   0.7964\n",
            "36256            843606578  healthcareicare  ...  0.061   0.0258\n",
            "36263  1096907821717827585         radler52  ...  0.135   0.4215\n",
            "\n",
            "[13570 rows x 14 columns]\n",
            "                   user_id             user  ...    pos compound\n",
            "2               3250859784          hartpa8  ...  0.000  -0.4019\n",
            "3       825084934113079296    aplebeianlife  ...  0.000  -0.8591\n",
            "5       951582944379879424  VickiJo54203063  ...  0.000  -0.8591\n",
            "7                564986563       hoofheart1  ...  0.000  -0.8591\n",
            "8      1263535557503848450   Eloise93833941  ...  0.000  -0.3252\n",
            "...                    ...              ...  ...    ...      ...\n",
            "36249            319658966    icculjeannie1  ...  0.000  -0.5719\n",
            "36253           3628078273       attagirl15  ...  0.000  -0.7717\n",
            "36260  1050722042750951429       MSmiles050  ...  0.000  -0.9001\n",
            "36261            309558781         KellyRek  ...  0.149  -0.4404\n",
            "36262   803731706352193536      Blackn40ish  ...  0.000  -0.5023\n",
            "\n",
            "[13389 rows x 14 columns]\n",
            "                   user_id             user  ...  pos compound\n",
            "0               3295393856      evens_jason  ...  0.0      0.0\n",
            "1      1432096972819681282  America4True777  ...  0.0      0.0\n",
            "6      1088585923321831424  JohnMil80462729  ...  0.0      0.0\n",
            "10               389871683       logancolin  ...  0.0      0.0\n",
            "16      918255789378342917      bob30285497  ...  0.0      0.0\n",
            "...                    ...              ...  ...  ...      ...\n",
            "36254            253049000    BarryCPearson  ...  0.0      0.0\n",
            "36255   801117072835088384        Alexa2B74  ...  0.0      0.0\n",
            "36257           1345940180  PaulSorrentino3  ...  0.0      0.0\n",
            "36258             23545095    ChadLivengood  ...  0.0      0.0\n",
            "36259            109802310         FLAHAULT  ...  0.0      0.0\n",
            "\n",
            "[9305 rows x 14 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kwi9G0GGeB_y"
      },
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "def remove_stopwords(texts, common_words):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words and word not in common_words] for doc in texts]\n",
        "\n",
        "def topic_model(tweets_var):\n",
        "    tweet_sentences = tweets_var['clean_text'].values.tolist()\n",
        "    tweet_words = list(sent_to_words(tweet_sentences))\n",
        "    # remove stop words\n",
        "    common_words = ['covid','get','amp','one','many','us','americans','still','getting','like','via','new','no','going','got','also',\\\n",
        "                    'even','said','would','says','go','everyone','see','since','th','much','man','make','oh','co','saying','anyone',\\\n",
        "                    'tell','mean','two','according','america','sure','use','look','come','around','almost','done','today','give','everyone','ever']\n",
        "    tweet_words = remove_stopwords(tweet_words, common_words)\n",
        "\n",
        "\n",
        "    # Create Dictionary\n",
        "    id2word = corpora.Dictionary(tweet_words)\n",
        "    # Create Corpus\n",
        "    texts = tweet_words\n",
        "    # Term Document Frequency\n",
        "    corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "    # number of topics\n",
        "    num_topics = 10\n",
        "    # Build LDA model\n",
        "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=num_topics)\n",
        "    doc_lda = lda_model[corpus]\n",
        "    return doc_lda, lda_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "itkvlPbw-G4E",
        "outputId": "6d66f8c4-a9f7-4100-d88f-176d53fe5834"
      },
      "source": [
        "tweet_sentences = tweets_neg['clean_text'].values.tolist()\n",
        "tweet_words = list(sent_to_words(tweet_sentences))\n",
        "tweet_words = remove_stopwords(tweet_words)\n",
        "x = pd.DataFrame([tweet_words]).T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-091df4c68458>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweet_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets_neg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtweet_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtweet_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tweets_neg' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nMFMEui_GHR"
      },
      "source": [
        "words_set = set(tuple(tweet) for tweet in tweet_words)\n",
        "tweet_words_uq = [ list(tweet) for tweet in words_set ]\n",
        "tweets_words_uq_df = pd.DataFrame([tweet_words_uq]).T\n",
        "uq_words_count = []\n",
        "count_temp = []\n",
        "for tweet in tweets_words_uq_df[0]:\n",
        "  for word in tweet:\n",
        "    if word not in uq_words_count:\n",
        "      uq_words_count.append(word)\n",
        "      count_temp.append(1)\n",
        "    else:\n",
        "      count_temp[uq_words_count.index(word)] += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JT2qQYBCWgl",
        "outputId": "b3e7c90b-3e4d-4b50-c550-e1af9976c3de"
      },
      "source": [
        "uq_words_count_df = pd.DataFrame([uq_words_count]).T\n",
        "uq_words_count_df[1] = np.array(count_temp)\n",
        "uq_words_count_df.rename(columns={0: 'Words', 1: 'Count'}, inplace=True)\n",
        "uq_words_count_df.sort_values(by='Count',inplace=True,ascending=False)\n",
        "uq_words_count_df['Words'].iloc[151:200]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1456               well\n",
              "1227               sure\n",
              "1087         healthcare\n",
              "1212             trying\n",
              "973              likely\n",
              "706              states\n",
              "423               rates\n",
              "488                 use\n",
              "1283            country\n",
              "983                 low\n",
              "416               delta\n",
              "307                look\n",
              "442                 cov\n",
              "18               school\n",
              "1269               come\n",
              "786             illegal\n",
              "255              family\n",
              "755              report\n",
              "806             medical\n",
              "579     hospitalization\n",
              "285               kills\n",
              "345                back\n",
              "463               child\n",
              "651              around\n",
              "165            symptoms\n",
              "738                hoax\n",
              "166              almost\n",
              "524                done\n",
              "396                high\n",
              "478               sorry\n",
              "525           hospitals\n",
              "888               today\n",
              "1652              fully\n",
              "824              person\n",
              "825             doctors\n",
              "1754              texas\n",
              "1248               real\n",
              "134                show\n",
              "104                give\n",
              "1168           immunity\n",
              "805                keep\n",
              "642             schools\n",
              "374            response\n",
              "1001             issues\n",
              "2208               shit\n",
              "912               india\n",
              "868              enough\n",
              "649           dangerous\n",
              "2001               ever\n",
              "Name: Words, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xojKAc985iWE"
      },
      "source": [
        "doc_lda_pos, lda_model_pos = topic_model(tweets_pos)\n",
        "doc_lda_neg, lda_model_neg = topic_model(tweets_neg)\n",
        "doc_lda_neu, lda_model_neu = topic_model(tweets_neu)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIr5qNnig4rI"
      },
      "source": [
        "def strip_string(string):\n",
        "  string = string.lstrip(\"' \")\n",
        "  string = string.rstrip(\"\\\" \")\n",
        "  string = string.split(\"*\\\"\")\n",
        "  return string\n",
        "\n",
        "def topics_populate(name,topics_df,lda):\n",
        "  for i in range(lda.num_topics):\n",
        "    list_terms = list(map(strip_string,lda.print_topics()[i][1].split('+')))\n",
        "    for j in range(len(list_terms)):\n",
        "      if list_terms[j][1] not in topics_df.index.tolist():\n",
        "        new_row = np.zeros(lda.num_topics)\n",
        "        new_row[i] = list_terms[j][0]\n",
        "        new_row = pd.Series(new_row)\n",
        "        new_row.name = list_terms[j][1]\n",
        "        topics_df = topics_df.append(new_row)\n",
        "      else:\n",
        "        topics_df.loc[list_terms[j][1],i] = list_terms[j][0]\n",
        "\n",
        "  #topics_df = topics_df.drop(['vaccine','covid'])\n",
        "  topics_df.to_csv(r'/content/drive/MyDrive/UT Austin/College/Fall/Unstructured/Project/{0}.csv'.format(name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfzWqOSf7RIk"
      },
      "source": [
        "topics_pos = pd.DataFrame()\n",
        "topics_neg = pd.DataFrame()\n",
        "topics_neu = pd.DataFrame()\n",
        "\n",
        "#topics_populate('topics_pos',topics_pos,lda_model_pos)\n",
        "#topics_populate('topics_neg',topics_neg,lda_model_neg)\n",
        "#topics_populate('topics_neu',topics_neu,lda_model_neu)\n",
        "topics_populate('mentions_topics_pos',topics_pos,lda_model_pos)\n",
        "topics_populate('mentions_topics_neg',topics_neg,lda_model_neg)\n",
        "topics_populate('mentions_topics_neu',topics_neu,lda_model_neu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcT9jYFqy0HC"
      },
      "source": [
        "def add_topics_to_tweets(name,topics_df,tweets_var,doc):\n",
        "  topic_scores = [[topic_score[1] for topic_score in tweet] for tweet in doc]\n",
        "  df_topics = pd.DataFrame(topic_scores)\n",
        "  max_topics = [max(tweet, key=lambda x: x[1])[0] for tweet in doc]\n",
        "  max_topics_df = pd.DataFrame(max_topics)\n",
        "\n",
        "  tweets_var.reset_index(inplace=True)\n",
        "  tweets_var.drop('index',axis=1,inplace=True)\n",
        "  tweets_var['topic_id'] = max_topics_df\n",
        "\n",
        "  topics_df = pd.read_csv(r'/content/drive/MyDrive/UT Austin/College/Fall/Unstructured/Project/{0}.csv'.format(name))\n",
        "  topics_df.set_index('Unnamed: 0',inplace=True)\n",
        "  topics_df = topics_df.columns.to_frame().reset_index()\n",
        "\n",
        "  tweets = tweets_var.join(topics_df[0], how='left',on='topic_id')\n",
        "  tweets.rename({0:'topic'},axis=1,inplace=True)\n",
        "  tweets.to_csv(r'/content/drive/MyDrive/UT Austin/College/Fall/Unstructured/Project/{0}_final.csv'.format(name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPw73w3X8hdq",
        "outputId": "19bd48ac-63fb-41e5-c36b-78645a4e3769"
      },
      "source": [
        "#add_topics_to_tweets('topics_pos',topics_pos,tweets_pos,doc_lda_pos)\n",
        "#add_topics_to_tweets('topics_neg',topics_neg,tweets_neg,doc_lda_neg)\n",
        "#add_topics_to_tweets('topics_neu',topics_neu,tweets_neu,doc_lda_neu)\n",
        "add_topics_to_tweets('mentions_topics_pos',topics_pos,tweets_pos,doc_lda_pos)\n",
        "add_topics_to_tweets('mentions_topics_neg',topics_neg,tweets_neg,doc_lda_neg)\n",
        "add_topics_to_tweets('mentions_topics_neu',topics_neu,tweets_neu,doc_lda_neu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ]
    }
  ]
}